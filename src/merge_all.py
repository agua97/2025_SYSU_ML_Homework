import pandas as pd
import pickle
import os
import numpy as np
import random

# ==================== ğŸ“‚ è·¯å¾„é…ç½® ====================
# 1. åŸå§‹æ•°æ®ç›®å½•
BASE_DATA_DIR = r'../Agent4Rec-master/datasets/ml-1m/cf_data'
# 2. Agent ä»¿çœŸæ—¥å¿—ç›®å½•
PKL_FOLDER_PATH = r'..\Agent4Rec-master\storage\ml-1m\LightGCN\lgn_1000_5_4_1009\behavior_clean'
# 3. è¾“å‡ºç›®å½•
RECHORUS_DATA_ROOT = '../data'
# ====================================================

def read_cf_txt(filepath):
    print(f"ğŸ“– è¯»å–åŸå§‹æ–‡ä»¶: {filepath}")
    data = []
    if not os.path.exists(filepath): return pd.DataFrame(), {}
    user_history = {} 
    with open(filepath, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) < 2: continue
            user_id = int(parts[0])
            items = [int(i) for i in parts[1:]]
            if user_id not in user_history: user_history[user_id] = set()
            user_history[user_id].update(items)
            for item in items: data.append([user_id, item])
    return pd.DataFrame(data, columns=['user_id', 'item_id']), user_history

def parse_agent_data_all(folder_path):
    """
    ã€Variant A æ ¸å¿ƒé€»è¾‘ã€‘
    æå–æ‰€æœ‰ watch_idï¼Œä¸ç®¡è¯„åˆ†æ˜¯å¤šå°‘ï¼Œå…¨éƒ¨ä¿ç•™ã€‚
    """
    print(f"ğŸ¤– è§£æ Agent æ•°æ® (å…¨é‡æ— æ¸…æ´—æ¨¡å¼)...")
    new_interactions = []
    if not os.path.exists(folder_path): return pd.DataFrame()

    files = [f for f in os.listdir(folder_path) if f.endswith('.pkl')]
    for filename in files:
        try:
            user_id = int(filename.split('.')[0])
            with open(os.path.join(folder_path, filename), 'rb') as f:
                data = pickle.load(f)
            
            if isinstance(data, dict):
                for page, content in data.items():
                    # ç›´æ¥è·å–çœ‹è¿‡çš„åˆ—è¡¨
                    watch_ids = content.get('watch_id', [])
                    
                    if isinstance(watch_ids, (np.ndarray, list)):
                        for i in range(len(watch_ids)):
                            # åªè¦çœ‹äº†å°±åŠ è¿›å»ï¼Œä¸åˆ¤æ–­ rating
                            new_interactions.append([user_id, int(watch_ids[i])])
        except: continue
    return pd.DataFrame(new_interactions, columns=['user_id', 'item_id'])

def generate_negative_samples(df_target, global_history, all_items, num_neg=99):
    print(f"ğŸ² ç”Ÿæˆè´Ÿæ ·æœ¬...")
    neg_lists = []
    all_items_list = list(all_items)
    for idx, row in df_target.iterrows():
        u, i = row['user_id'], row['item_id']
        seen = global_history.get(u, set())
        seen.add(i)
        samples = []
        while len(samples) < num_neg:
            candidates = random.sample(all_items_list, min(len(all_items_list), num_neg * 2))
            for cand in candidates:
                if cand not in seen and cand not in samples:
                    samples.append(cand)
                    if len(samples) == num_neg: break
        neg_lists.append(str(samples))
    return neg_lists

def save_dataset(df_train, df_valid, df_test, folder_name):
    target_dir = os.path.join(RECHORUS_DATA_ROOT, folder_name)
    if not os.path.exists(target_dir): os.makedirs(target_dir)
    print(f"ğŸ’¾ ä¿å­˜æ•°æ®é›†è‡³: {target_dir}")
    
    df_train[['user_id', 'item_id', 'time']].to_csv(
        os.path.join(target_dir, 'train.csv'), sep='\t', index=False, header=['user_id', 'item_id', 'time'])
    
    df_valid[['user_id', 'item_id', 'time', 'neg_items']].to_csv(
        os.path.join(target_dir, 'dev.csv'), sep='\t', index=False, header=['user_id', 'item_id', 'time', 'neg_items'])
    
    df_test[['user_id', 'item_id', 'time', 'neg_items']].to_csv(
        os.path.join(target_dir, 'test.csv'), sep='\t', index=False, header=['user_id', 'item_id', 'time', 'neg_items'])

def main():
    # 1. è¯»å–åŸå§‹æ•°æ®
    df_train_orig, hist_train = read_cf_txt(os.path.join(BASE_DATA_DIR, 'train.txt'))
    df_valid_orig, hist_valid = read_cf_txt(os.path.join(BASE_DATA_DIR, 'valid.txt'))
    df_test_orig, hist_test  = read_cf_txt(os.path.join(BASE_DATA_DIR, 'test.txt'))
    
    # è¡¥å……æ—¶é—´æˆ³ (ReChoruså¿…éœ€)
    df_train_orig['time'] = 1
    df_valid_orig['time'] = 2
    df_test_orig['time'] = 3

    # 2. è¯»å– Agent å…¨é‡æ•°æ® (æ— æ¸…æ´—)
    df_agent_all = parse_agent_data_all(PKL_FOLDER_PATH)
    df_agent_all['time'] = 1
    
    print(f"åŸå§‹è®­ç»ƒé›†æ•°é‡: {len(df_train_orig)}")
    print(f"Agentæ–°å¢æ•°é‡ (Variant A): {len(df_agent_all)}")

    # 3. åˆå¹¶è®­ç»ƒé›† (Origin + Agent All)
    # å»é‡ï¼šé˜²æ­¢ Agent çœ‹äº†ç”¨æˆ·æœ¬æ¥å°±çœ‹è¿‡çš„ç”µå½±
    df_train_variant_a = pd.concat([df_train_orig, df_agent_all], ignore_index=True)
    df_train_variant_a.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)

    # 4. ç”Ÿæˆè´Ÿæ ·æœ¬ (Dev/Test)
    # æ³¨æ„ï¼šè´Ÿæ ·æœ¬å¿…é¡»é¿å¼€ (è®­ç»ƒé›† + Agentå…¨é‡æ•°æ® + éªŒè¯æµ‹è¯•é›†)
    all_items = set()
    all_items.update(df_train_variant_a['item_id'].unique())
    all_items.update(df_valid_orig['item_id'].unique())
    all_items.update(df_test_orig['item_id'].unique())
    
    global_hist = {}
    def merge_h(h):
        for u, items in h.items():
            if u not in global_hist: global_hist[u] = set()
            global_hist[u].update(items)
    merge_h(hist_train); merge_h(hist_valid); merge_h(hist_test)
    
    # æŠŠ Agent çš„æ•°æ®ä¹ŸåŠ å…¥å†å²ï¼Œé˜²æ­¢è´Ÿæ ·æœ¬é‡‡åˆ°
    for idx, row in df_agent_all.iterrows():
        u, i = row['user_id'], row['item_id']
        if u not in global_hist: global_hist[u] = set()
        global_hist[u].add(i)

    # ç”Ÿæˆè´Ÿæ ·æœ¬
    df_valid_orig['neg_items'] = generate_negative_samples(df_valid_orig, global_hist, all_items)
    df_test_orig['neg_items'] = generate_negative_samples(df_test_orig, global_hist, all_items)

    # 5. ä¿å­˜
    save_dataset(df_train_variant_a, df_valid_orig, df_test_orig, 'Agent4Rec_All')
    
    print("âœ… å®Œæˆï¼è¯·ä½¿ç”¨æ•°æ®é›† 'Agent4Rec_All' è¿è¡Œæ¶ˆèå®éªŒã€‚")

if __name__ == '__main__':
    main()