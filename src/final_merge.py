import pandas as pd
import pickle
import os
import numpy as np
import random
import ast

# ==================== ğŸ“‚ è·¯å¾„é…ç½® ====================
# 1. Agent4Rec çš„åŸºå‡†æ•°æ®ç›®å½•
BASE_DATA_DIR = r'../Agent4Rec-master/datasets/ml-1m/cf_data'

# 2. Agent ä»¿çœŸæ•°æ®çš„ pkl æ–‡ä»¶å¤¹
PKL_FOLDER_PATH = r'..\Agent4Rec-master\storage\ml-1m\LightGCN\lgn_1000_5_4_1009\behavior_clean'

# 3. ReChorus çš„æ•°æ®æ ¹ç›®å½•
RECHORUS_DATA_ROOT = '../data'
# ====================================================

def read_cf_txt(filepath):
    """è¯»å– txt é‚»æ¥è¡¨æ–‡ä»¶"""
    print(f"ğŸ“– è¯»å–æ–‡ä»¶: {filepath}")
    data = []
    if not os.path.exists(filepath):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {filepath}")
        return pd.DataFrame(), {}
    
    # ç”¨å­—å…¸è®°å½•æ¯ä¸ªç”¨æˆ·çš„äº¤äº’å†å²ï¼Œç”¨äºè´Ÿé‡‡æ ·å»é‡
    user_history = {} 
    
    with open(filepath, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) < 2: continue
            
            user_id = int(parts[0])
            items = [int(i) for i in parts[1:]]
            
            # è®°å½•å†å²
            if user_id not in user_history:
                user_history[user_id] = set()
            user_history[user_id].update(items)
            
            for item in items:
                data.append([user_id, item])
                
    df = pd.DataFrame(data, columns=['user_id', 'item_id'])
    return df, user_history

def parse_agent_viewed_data(folder_path):
    """è§£æ Agent çš„ pkl æ—¥å¿—"""
    print(f"ğŸ¤– è§£æ Agent ä»¿çœŸæ•°æ®: {folder_path}")
    new_interactions = []
    
    if not os.path.exists(folder_path):
        return pd.DataFrame()

    files = [f for f in os.listdir(folder_path) if f.endswith('.pkl')]
    
    for filename in files:
        try:
            user_id = int(filename.split('.')[0])
            with open(os.path.join(folder_path, filename), 'rb') as f:
                data = pickle.load(f)
            
            if isinstance(data, dict):
                for page, content in data.items():
                    watch_ids = content.get('watch_id', [])
                    ratings = content.get('rating', [])
                    if isinstance(watch_ids, (np.ndarray, list)):
                        for i in range(len(watch_ids)):
                         # åªæœ‰å½“è¯„åˆ†å­˜åœ¨ä¸” >= 4 æ—¶æ‰åŠ å…¥
                            if i < len(ratings) and int(ratings[i]) >= 4:
                                new_interactions.append([user_id, int(watch_ids[i])])
        except Exception:
            continue
            
    df = pd.DataFrame(new_interactions, columns=['user_id', 'item_id'])
    return df

def generate_negative_samples(df_target, global_history, all_items, num_neg=99):
    """
    æ ¸å¿ƒå‡½æ•°ï¼šä¸ºæµ‹è¯•é›†/éªŒè¯é›†çš„æ¯ä¸€è¡Œç”Ÿæˆ 99 ä¸ªè´Ÿæ ·æœ¬
    """
    print(f"ğŸ² æ­£åœ¨ä¸º {len(df_target)} æ¡æ•°æ®ç”Ÿæˆ 99 ä¸ªè´Ÿæ ·æœ¬ (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
    
    neg_lists = []
    all_items_list = list(all_items) # è½¬æˆåˆ—è¡¨ä»¥ä¾¿é‡‡æ ·
    
    for idx, row in df_target.iterrows():
        u = row['user_id']
        i = row['item_id']
        
        # è¯¥ç”¨æˆ·çœ‹è¿‡çš„æ‰€æœ‰ç”µå½± (Train + Valid + Test + AgentHistory)
        seen = global_history.get(u, set())
        # è¿˜è¦åŠ ä¸Šå½“å‰è¿™ä¸€æ¡æµ‹è¯•æ•°æ®çš„ item (é˜²æ­¢æ¼æ‰)
        seen.add(i)
        
        samples = []
        while len(samples) < num_neg:
            candidates = random.sample(all_items_list, min(len(all_items_list), num_neg * 2))
            for cand in candidates:
                if cand not in seen and cand not in samples:
                    samples.append(cand)
                    if len(samples) == num_neg:
                        break
        
        # è½¬æˆå­—ç¬¦ä¸²æ ¼å¼ "[1, 2, 3]" æ–¹ä¾¿ csv ä¿å­˜
        neg_lists.append(str(samples))
        
    return neg_lists

def save_to_folder(df_train, df_valid, df_test, folder_name):
    target_dir = os.path.join(RECHORUS_DATA_ROOT, folder_name)
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)
        
    print(f"ğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ°: {target_dir}")
    
    # ä¿å­˜ Train (ä¸éœ€è¦è´Ÿæ ·æœ¬)
    df_train = df_train[['user_id', 'item_id', 'time']]
    df_train.to_csv(os.path.join(target_dir, 'train.csv'), sep='\t', index=False, header=['user_id', 'item_id', 'time'])
    
    # ä¿å­˜ Dev (å¸¦è´Ÿæ ·æœ¬)
    df_valid = df_valid[['user_id', 'item_id', 'time', 'neg_items']]
    df_valid.to_csv(os.path.join(target_dir, 'dev.csv'), sep='\t', index=False, header=['user_id', 'item_id', 'time', 'neg_items'])
    
    # ä¿å­˜ Test (å¸¦è´Ÿæ ·æœ¬)
    df_test = df_test[['user_id', 'item_id', 'time', 'neg_items']]
    df_test.to_csv(os.path.join(target_dir, 'test.csv'), sep='\t', index=False, header=['user_id', 'item_id', 'time', 'neg_items'])
    
    print(f"   Train: {len(df_train)} | Dev: {len(df_valid)} | Test: {len(df_test)}")

def main():
    # 1. è¯»å–æ‰€æœ‰åŸå§‹æ•°æ®
    print("â³ [Step 1] è¯»å–åŸå§‹æ•°æ®...")
    df_train_orig, hist_train = read_cf_txt(os.path.join(BASE_DATA_DIR, 'train.txt'))
    df_valid_orig, hist_valid = read_cf_txt(os.path.join(BASE_DATA_DIR, 'valid.txt'))
    df_test_orig, hist_test  = read_cf_txt(os.path.join(BASE_DATA_DIR, 'test.txt'))
    
    # 2. è¯»å– Agent æ•°æ®
    print("\nâ³ [Step 2] è¯»å– Agent æ•°æ®...")
    df_agent = parse_agent_viewed_data(PKL_FOLDER_PATH)
    
    # =============== ğŸ”´ æ–°å¢ï¼šä¸¥æ ¼è¿‡æ»¤é€»è¾‘ (å¤ç°è®ºæ–‡) ===============
    print("\nâœ‚ï¸ [Filter] æ­£åœ¨è¿‡æ»¤é Simulation ç”¨æˆ·...")
    
    # 1. è·å–é‚£ 1000 ä¸ªæ¨¡æ‹Ÿç”¨æˆ·çš„ ID åˆ—è¡¨
    target_users = df_agent['user_id'].unique()
    target_uid_set = set(target_users)
    print(f"   æ£€æµ‹åˆ°ä»¿çœŸç”¨æˆ·æ•°: {len(target_uid_set)}")

    # 2. è¿‡æ»¤åŸå§‹æ•°æ®é›†ï¼Œåªä¿ç•™è¿™ 1000 äºº
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¦†ç›–åŸå˜é‡ï¼ŒæŠŠä¸ç›¸å…³çš„äººå‰”é™¤
    df_train_orig = df_train_orig[df_train_orig['user_id'].isin(target_uid_set)].copy()
    df_valid_orig = df_valid_orig[df_valid_orig['user_id'].isin(target_uid_set)].copy()
    df_test_orig  = df_test_orig[df_test_orig['user_id'].isin(target_uid_set)].copy()

    # 3. å¿…é¡»åŒæ­¥è¿‡æ»¤ hist (å¦åˆ™è´Ÿé‡‡æ ·ä¼šå‡ºé”™)
    # é‡æ–°æ„å»ºåªåŒ…å«è¿™ 1000 äººçš„å†å²å­—å…¸
    def filter_hist(old_hist):
        return {u: items for u, items in old_hist.items() if u in target_uid_set}

    hist_train = filter_hist(hist_train)
    hist_valid = filter_hist(hist_valid)
    hist_test = filter_hist(hist_test)

    print(f"   è¿‡æ»¤åå‰©ä½™è®°å½•æ•° -> Train: {len(df_train_orig)} | Valid: {len(df_valid_orig)} | Test: {len(df_test_orig)}")
    # ==============================================================
    
    
    # 3. æ„å»ºå…¨å±€ Item é›†åˆ å’Œ å…¨å±€ User History (ç”¨äºè´Ÿé‡‡æ ·å»é‡)
    print("\nğŸ—ï¸ [Step 3] æ„å»ºå…¨å±€ç´¢å¼•...")
    # è·å–æ‰€æœ‰å‡ºç°è¿‡çš„ Item ID
    all_items = set()
    all_items.update(df_train_orig['item_id'].unique())
    all_items.update(df_valid_orig['item_id'].unique())
    all_items.update(df_test_orig['item_id'].unique())
    all_items.update(df_agent['item_id'].unique()) # Agent æ–°å‘ç°çš„ç‰©å“ä¹Ÿç®—
    
    print(f"   ç³»ç»Ÿä¸­å…±æœ‰ {len(all_items)} ä¸ªä¸åŒçš„ Itemã€‚")
    
    # åˆå¹¶ç”¨æˆ·å†å²ï¼šHistory = Train + Valid + Test + AgentViewed
    # è¿™æ ·åœ¨é‡‡æ ·è´Ÿæ ·æœ¬æ—¶ï¼Œç»å¯¹ä¸ä¼šé‡‡åˆ°ç”¨æˆ·ä»¥å‰çœ‹è¿‡çš„
    global_history = {}
    
    # è¾…åŠ©å‡½æ•°ï¼šåˆå¹¶ history
    def merge_hist(source_hist):
        for u, items in source_hist.items():
            if u not in global_history: global_history[u] = set()
            global_history[u].update(items)
    
    merge_hist(hist_train)
    merge_hist(hist_valid)
    merge_hist(hist_test)
    
    # æŠŠ Agent çš„æ•°æ®ä¹ŸåŠ è¿›å†å² (é’ˆå¯¹ Enhanced ç»„ï¼Œä½†ä¸ºäº†æ–¹ä¾¿ï¼ŒBaseline ç»„ä¹Ÿå¯ä»¥å…±ç”¨è¿™ä¸ªæ’é™¤é€»è¾‘ï¼Œå› ä¸ºåæ­£æœ¬æ¥å°±æ²¡çœ‹è¿‡)
    for idx, row in df_agent.iterrows():
        u = row['user_id']
        i = row['item_id']
        if u not in global_history: global_history[u] = set()
        global_history[u].add(i)

    # 4. ä¸º Dev å’Œ Test ç”Ÿæˆè´Ÿæ ·æœ¬ (è®¡ç®—é‡è¾ƒå¤§ï¼Œåšä¸€æ¬¡å³å¯)
    print("\nğŸ² [Step 4] ç”Ÿæˆè´Ÿæ ·æœ¬ (99ä¸ª/æ¡)...")
    neg_valid = generate_negative_samples(df_valid_orig, global_history, all_items)
    neg_test = generate_negative_samples(df_test_orig, global_history, all_items)
    
    # å°†è´Ÿæ ·æœ¬æŒ‚è½½åˆ° DataFrame
    df_valid_orig['neg_items'] = neg_valid
    df_test_orig['neg_items'] = neg_test
    
    # è¡¥å…… time åˆ—
    df_train_orig['time'] = 1
    df_valid_orig['time'] = 2
    df_test_orig['time'] = 3
    df_agent['time'] = 1

    # 5. ä¿å­˜ Baseline
    print("\nğŸ“¦ [Step 5] ä¿å­˜ Baseline æ•°æ®é›†...")
    save_to_folder(df_train_orig, df_valid_orig, df_test_orig, 'AgentRec_Original')

    # 6. ä¿å­˜ Enhanced
    print("\nğŸ“¦ [Step 6] ä¿å­˜ Enhanced æ•°æ®é›†...")
    # åˆå¹¶ Agent æ•°æ®åˆ°è®­ç»ƒé›†
    df_train_enhanced = pd.concat([df_train_orig, df_agent], ignore_index=True)
    df_train_enhanced.drop_duplicates(subset=['user_id', 'item_id'], inplace=True)
    
    # Dev å’Œ Test ä¿æŒä¸å˜ (åŒ…å«åˆšæ‰ç”Ÿæˆçš„ neg_items)
    save_to_folder(df_train_enhanced, df_valid_orig, df_test_orig, 'AgentRec_Enhanced')

    print("\nğŸ‰ å®Œç¾è§£å†³ï¼ç°åœ¨ dev.csv å’Œ test.csv é‡Œé¢åŒ…å«äº†çœŸå®çš„ 99 ä¸ªè´Ÿæ ·æœ¬åˆ—è¡¨ã€‚")
    print("æ ¼å¼ç¤ºä¾‹: \"[120, 45, 999, ...]\"")

if __name__ == '__main__':
    main()